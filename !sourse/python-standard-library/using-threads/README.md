# [How to Use Threads to Speed Up your IO Tasks in Python](https://www.thepythoncode.com/article/using-threads-in-python)
To run this:
- `pip3 install -r requirements.txt`
##
# [[] / []]()
В вычислительной технике поток — это последовательность запрограммированных инструкций, выполняемых в программе, два потока, работающие в одной программе, означают, что они работают одновременно (т.е. а не параллельно). В отличие от процессов, потоки в Python не выполняются на отдельном ядре ЦП, они совместно используют пространство памяти и эффективно считывают и записывают в одни и те же переменные.

Потоки похожи на мини-процессы, на самом деле, некоторые люди называют их легкими процессами, потому что потоки могут жить внутри процесса и выполнять работу процесса. Но на самом деле они совсем другие, вот основные отличия между потоками и процессами в Python:

Процесс
Процесс может содержать один или несколько потоков
Процессы имеют отдельные пространства памяти
Два процесса могут работать на разных ядрах ЦП (что приводит к проблемам со связью, но лучшей производительности ЦП)
Процессы имеют больше накладных расходов, чем потоки (создание и уничтожение процессов занимает больше времени)
Запуск нескольких процессов эффективен только для задач, связанных с ЦП
Нить
Потоки используют одно и то же пространство памяти и могут считывать и записывать в общие переменные (с синхронизацией, конечно)
Два потока в одной программе Python не могут выполняться одновременно
Запуск нескольких потоков эффективен только для задач ввода-вывода
Теперь вы, возможно, задаетесь вопросом, зачем нам использовать потоки, если они не могут работать одновременно? Прежде чем мы ответим на этот вопрос, давайте посмотрим, почему потоки не могут работать одновременно.

ГИЛ Питона
Самая спорная тема среди разработчиков Python, GIL расшифровывается как Global Interpreter Lock, который в основном является блокировкой, которая предотвращает одновременное выполнение двух потоков в одном интерпретаторе Python, некоторым людям это не нравится, в то время как другие утверждают, что это не проблема, так как есть библиотеки, такие как Numpy, которые обходят это ограничение, запуская программы во внешнем коде C.

Теперь, зачем нам тогда использовать потоки? Ну, Python освобождает блокировку, ожидая разрешения блока ввода-вывода, поэтому, если ваш код Python делает запрос к API, базе данных на диске или загрузке из Интернета, Python не дает ему шанса даже получить блокировку, так как такие операции происходят за пределами GIL. Короче говоря, мы извлекаем выгоду только из потоков в привязке ввода-вывода.

Одиночная резьба
Для демонстрации приведенный ниже код пытается загрузить некоторые файлы из Интернета (что является идеальной задачей ввода-вывода) последовательно без использования потоков (он требует установки запросов, просто выполните запросы на установку pip3):

import requests
from time import perf_counter

# read 1024 bytes every time 
buffer_size = 1024

def download(url):
    # download the body of response by chunk, not immediately
    response = requests.get(url, stream=True)
    # get the file name
    filename = url.split("/")[-1]
    with open(filename, "wb") as f:
        for data in response.iter_content(buffer_size):
            # write data read to the file
            f.write(data)

if __name__ == "__main__":
    urls = [
        "https://cdn.pixabay.com/photo/2018/01/14/23/12/nature-3082832__340.jpg",
        "https://cdn.pixabay.com/photo/2013/10/02/23/03/dawn-190055__340.jpg",
        "https://cdn.pixabay.com/photo/2016/10/21/14/50/plouzane-1758197__340.jpg",
        "https://cdn.pixabay.com/photo/2016/11/29/05/45/astronomy-1867616__340.jpg",
        "https://cdn.pixabay.com/photo/2014/07/28/20/39/landscape-404072__340.jpg",
    ] * 5

    t = perf_counter()
    for url in urls:
        download(url)
    print(f"Time took: {perf_counter() - t:.2f}s")
Как только вы выполните его, вы заметите, что новые изображения появляются в текущем каталоге, и вы получите что-то вроде этого в качестве выходных данных:

Time took: 13.76s
Таким образом, приведенный выше код довольно прост, он перебирает эти изображения и загружает их каждое по одному, что заняло около 13,8 с (будет варьироваться в зависимости от вашего интернет-соединения), но в любом случае мы тратим много времени здесь, если вам нужна производительность, рассмотрите возможность использования потоков.

Связанные с: Асинхронные задачи с сельдереем в Python.

Несколько потоков
import requests
from concurrent.futures import ThreadPoolExecutor
from time import perf_counter

# number of threads to spawn
n_threads = 5
# read 1024 bytes every time 
buffer_size = 1024

def download(url):
    # download the body of response by chunk, not immediately
    response = requests.get(url, stream=True)
    # get the file name
    filename = url.split("/")[-1]
    with open(filename, "wb") as f:
        for data in response.iter_content(buffer_size):
            # write data read to the file
            f.write(data)

if __name__ == "__main__":
    urls = [
        "https://cdn.pixabay.com/photo/2018/01/14/23/12/nature-3082832__340.jpg",
        "https://cdn.pixabay.com/photo/2013/10/02/23/03/dawn-190055__340.jpg",
        "https://cdn.pixabay.com/photo/2016/10/21/14/50/plouzane-1758197__340.jpg",
        "https://cdn.pixabay.com/photo/2016/11/29/05/45/astronomy-1867616__340.jpg",
        "https://cdn.pixabay.com/photo/2014/07/28/20/39/landscape-404072__340.jpg",
    ] * 5
    t = perf_counter()
    with ThreadPoolExecutor(max_workers=n_threads) as pool:
        pool.map(download, urls)
    print(f"Time took: {perf_counter() - t:.2f}s")
Код здесь немного изменен, теперь мы используем класс ThreadPoolExecutor из пакета concurrent.futures, он в основном создает пул с несколькими потоками, которые мы указываем, а затем обрабатывает разделение списка URL-адресов по потокам с помощью метода pool.map().

Вот как долго это продолжалось для меня:

Time took: 3.85s
То есть примерно на x3.6 быстрее (по крайней мере, для меня), используя 5 потоков, попробуйте настроить количество потоков, которые будут появляться на вашей машине, и посмотрите, сможете ли вы его дополнительно оптимизировать.

Теперь это не единственный способ создания потоков, вы также можете использовать удобный модуль потоков с очередью, вот еще один эквивалентный код:

import requests
from threading import Thread
from queue import Queue

# thread-safe queue initialization
q = Queue()
# number of threads to spawn
n_threads = 5
# read 1024 bytes every time 
buffer_size = 1024

def download():
    global q
    while True:
        # get the url from the queue
        url = q.get()
        # download the body of response by chunk, not immediately
        response = requests.get(url, stream=True)
        # get the file name
        filename = url.split("/")[-1]
        with open(filename, "wb") as f:
            for data in response.iter_content(buffer_size):
                # write data read to the file
                f.write(data)
        # we're done downloading the file
        q.task_done()

if __name__ == "__main__":
    urls = [
        "https://cdn.pixabay.com/photo/2018/01/14/23/12/nature-3082832__340.jpg",
        "https://cdn.pixabay.com/photo/2013/10/02/23/03/dawn-190055__340.jpg",
        "https://cdn.pixabay.com/photo/2016/10/21/14/50/plouzane-1758197__340.jpg",
        "https://cdn.pixabay.com/photo/2016/11/29/05/45/astronomy-1867616__340.jpg",
        "https://cdn.pixabay.com/photo/2014/07/28/20/39/landscape-404072__340.jpg",
    ] * 5
    # fill the queue with all the urls
    for url in urls:
        q.put(url)
    # start the threads
    for t in range(n_threads):
        worker = Thread(target=download)
        # daemon thread means a thread that will end when the main thread ends
        worker.daemon = True
        worker.start()
    # wait until the queue is empty
    q.join()
Это также хорошая альтернатива, мы используем синхронизированную очередь здесь, в которой мы заполняем ее всеми URL-адресами изображений, которые мы хотим загрузить, а затем вручную порождаем потоки, каждый из которых выполняет функцию download().

Как вы, возможно, уже видели, функция download() использует бесконечный цикл, который никогда не закончится, я знаю, что это противоречит интуиции, но это имеет смысл, когда мы знаем, что поток, выполняющий эту функцию, является потоком демона, что означает, что он будет заканчиваться до тех пор, пока заканчивается основной поток.

Таким образом, мы используем метод q.put(), чтобы поместить элемент, и q.get(), чтобы получить этот элемент и потреблять его (в данном случае загрузить его), это проблема производителя-потребителя, которая широко обсуждается в области компьютерных наук.

Теперь, что, если два потока выполняют метод q.get() (или q.put() одновременно, что произойдет? Ну, я уже говорил, что эта очередь является потокобезопасной (синхронизированной), что означает, что она использует блокировку под капотом, которая не позволяет двум потокам получать элемент одновременно.

Когда мы завершаем загрузку этого файла, мы вызываем метод q.task_done(), который сообщает очереди, что обработка задачи (для этого элемента) завершена.

Возвращаясь к основному потоку, мы создали потоки и запустили их с помощью метода start(), после этого нам нужен способ блокировать основной поток до тех пор, пока все потоки не будут завершены, именно это и делает q.join(), он блокирует до тех пор, пока все элементы в очереди не будут получены и обработаны.

Заключение
В заключение, во-первых, вы не должны использовать потоки, если вам не нужно ускорять свой код, возможно, вы выполняете его один раз в месяц, поэтому вы просто добавите сложность кода, что может привести к трудностям на этапе отладки.

Во-вторых, если ваш код в значительной степени является задачей ЦП, вы также не должны использовать потоки, это из-за GIL. Если вы хотите запустить свой код на многоядерных ядрах, вам следует использовать многопроцессорный модуль, который обеспечивает аналогичную функциональность, но с процессами вместо потоков.

В-третьих, вы должны использовать потоки только для задач ввода-вывода, таких как запись на диск, ожидание сетевого ресурса и т. Д.

Наконец, вы должны использовать ThreadPoolExecutor, когда у вас есть элементы для обработки, прежде чем даже начать их использовать. Однако, если ваши элементы не предопределены и захватываются во время выполнения кода (как это обычно бывает в веб-парсинге), рассмотрите возможность использования синхронизированной очереди с модулем потоков.

Вот несколько учебников, в которых мы использовали темы:

Как сделать сканер поддоменов в Python
Как сделать сканер портов в Python с помощью библиотеки сокетов
Как написать кейлоггер на Python с нуля
Как создать поддельные точки доступа с помощью Scapy в Python