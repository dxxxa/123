# [How to Use Proxies to Anonymize your Browsing and Scraping using Python](https://www.thepythoncode.com/article/using-proxies-using-requests-in-python)
To run this:
- `pip3 install -r requirements.txt`
- If you want to use free available proxies, use `free_proxies.py`
- If you want to use Tor network, make sure Tor is installed in your machine and the service is running. `tor_proxy.py`
- If you want IP rotation on Tor, use `multiple_tor_proxies.py`
##
# [[] / []]()
Прокси - это серверное приложение, которое выступает в качестве посредника для запросов между клиентом и сервером, с которого клиент запрашивает определенную услугу (HTTP, SSL и т. Д.).

При использовании прокси-сервера, вместо того, чтобы напрямую подключаться к целевому серверу и запрашивать все, что вы хотите запросить, вы направляете запрос на прокси-сервер, который оценивает запрос и выполняет его, и возвращает ответ, вот простая демонстрация прокси-серверов в Википедии:



Эксперты по веб-парсингу часто используют более одного прокси-сервера, чтобы веб-сайты не запрещали свои IP-адреса. Прокси имеют несколько других преимуществ, включая обход фильтров и цензуры, сокрытие вашего реального IP-адреса и т. Д.

В этом уроке вы узнаете, как вы можете использовать прокси в Python с помощью библиотеки запросов, мы также будем использовать библиотеку stem, которая является библиотекой контроллера Python для Tor, давайте установим их:

pip3 install bs4 requests stem
Связанные с: Как сделать сканер поддоменов в Python.

Использование бесплатных доступных прокси
Во-первых, есть некоторые веб-сайты, которые предлагают бесплатный список прокси для использования, я создал функцию для автоматического захвата этого списка:

import requests
import random
from bs4 import BeautifulSoup as bs

def get_free_proxies():
    url = "https://free-proxy-list.net/"
    # get the HTTP response and construct soup object
    soup = bs(requests.get(url).content, "html.parser")
    proxies = []
    for row in soup.find("table", attrs={"id": "proxylisttable"}).find_all("tr")[1:]:
        tds = row.find_all("td")
        try:
            ip = tds[0].text.strip()
            port = tds[1].text.strip()
            host = f"{ip}:{port}"
            proxies.append(host)
        except IndexError:
            continue
    return proxies
Однако, когда я попытался их использовать, большинство из них были тайм-аутами, я отфильтровал некоторые рабочие:

proxies = [
    '167.172.248.53:3128',
    '194.226.34.132:5555',
    '203.202.245.62:80',
    '141.0.70.211:8080',
    '118.69.50.155:80',
    '201.55.164.177:3128',
    '51.15.166.107:3128',
    '91.205.218.64:80',
    '128.199.237.57:8080',
]
Этот список не может быть жизнеспособным вечно, на самом деле, большинство из них перестанут работать, когда вы прочитаете этот учебник (поэтому вы должны выполнять вышеуказанную функцию каждый раз, когда хотите использовать новые прокси-серверы).

Приведенная ниже функция принимает список прокси и создает сеанс запросов, который случайным образом выбирает один из переданных прокси:

def get_session(proxies):
    # construct an HTTP session
    session = requests.Session()
    # choose one random proxy
    proxy = random.choice(proxies)
    session.proxies = {"http": proxy, "https": proxy}
    return session
Давайте проверим это, сделав запрос на веб-сайт, который возвращает наш IP-адрес:

for i in range(5):
    s = get_session(proxies)
    try:
        print("Request page with IP:", s.get("http://icanhazip.com", timeout=1.5).text.strip())
    except Exception as e:
        continue
Вот мой вывод:

Request page with IP: 45.64.134.198
Request page with IP: 141.0.70.211
Request page with IP: 94.250.248.230
Request page with IP: 46.173.219.2
Request page with IP: 201.55.164.177
Как видите, это некоторые IP-адреса работающих прокси-серверов, а не наш реальный IP-адрес (попробуйте посетить этот сайт в своем браузере, и вы увидите свой реальный IP-адрес).

Бесплатные прокси, как правило, умирают очень быстро, в основном в течение нескольких дней или даже часов, и часто умирают до того, как закончится наш проект по скребку. Чтобы предотвратить это, вам нужно использовать премиум-прокси для крупномасштабных проектов извлечения данных, есть много провайдеров, которые меняют IP-адреса для вас. Одним из известных решений является Crawlera. Подробнее об этом мы поговорим в последнем разделе этого урока.

Использование Tor в качестве прокси
Вы также можете использовать сеть Tor для поворота IP-адресов:

import requests
from stem.control import Controller
from stem import Signal

def get_tor_session():
    # initialize a requests Session
    session = requests.Session()
    # setting the proxy of both http & https to the localhost:9050 
    # this requires a running Tor service in your machine and listening on port 9050 (by default)
    session.proxies = {"http": "socks5://localhost:9050", "https": "socks5://localhost:9050"}
    return session

def renew_connection():
    with Controller.from_port(port=9051) as c:
        c.authenticate()
        # send NEWNYM signal to establish a new clean connection through the Tor network
        c.signal(Signal.NEWNYM)

if __name__ == "__main__":
    s = get_tor_session()
    ip = s.get("http://icanhazip.com").text
    print("IP:", ip)
    renew_connection()
    s = get_tor_session()
    ip = s.get("http://icanhazip.com").text
    print("IP:", ip)
Заметка: Приведенный выше код должен работать только в том случае, если на вашем компьютере установлен Tor (перейдите по этой ссылке, чтобы правильно установить его) и хорошо настроен (ControlPort 9051 включен, проверьте этот ответ stackoverflow для получения дополнительной информации).

Это создаст сеанс с IP-адресом Tor и сделает HTTP-запрос, а затем обновит соединение, отправив сигнал NEWNYM (который говорит Tor установить новое чистое соединение), чтобы изменить IP-адрес и сделать еще один запрос, вот вывод:

IP: 185.220.101.49

IP: 109.70.100.21
Прекрасно! Однако, когда вы испытываете веб-парсинг с использованием сети Tor, вы скоро поймете, что он довольно медленный большую часть времени, поэтому рекомендуемый способ приведен ниже.

Использование Crawlera
Crawlera от Scrapinghub позволяет быстро и надежно сканировать, он управляет и вращает прокси-серверы внутри, поэтому, если вас забанили, он автоматически обнаружит это и повернет IP-адрес для вас.

Crawlera - это интеллектуальная прокси-сеть, специально разработанная для веб-парсинга и сканирования. Его работа ясна: сделать вашу жизнь проще в качестве веб-скребка. Это поможет вам получать успешные запросы и извлекать данные в масштабе с любого веб-сайта, используя любой инструмент веб-парсинга.

Кроллера Прокси

Благодаря простому API запрос, который вы делаете при очистке, будет направляться через пул высококачественных прокси. При необходимости он автоматически вводит задержки между запросами и удаляет / добавляет IP-адреса для преодоления различных проблем сканирования.

Вот как вы можете использовать Crawlera с библиотекой запросов в Python:

import requests

url = "http://icanhazip.com"
proxy_host = "proxy.crawlera.com"
proxy_port = "8010"
proxy_auth = "<APIKEY>:"
proxies = {
       "https": f"https://{proxy_auth}@{proxy_host}:{proxy_port}/",
       "http": f"http://{proxy_auth}@{proxy_host}:{proxy_port}/"
}

r = requests.get(url, proxies=proxies, verify=False)
Как только вы зарегистрируетесь для получения плана, вам будет предоставлен ключ API, в котором вы замените proxy_auth.

Итак, вот что Crawlera делает для вас:

HTTP-запрос отправляется с помощью API одной конечной точки.
Он автоматически выбирает, поворачивает, регулирует и зачеркивает IP-адреса для извлечения целевых данных.
Он обрабатывает заголовки запросов и поддерживает сеансы.
В ответ вы получите успешный запрос.
Заключение
Существует несколько типов прокси, включая прозрачные прокси, анонимные прокси и элитные прокси. Если ваша цель использования прокси - предотвратить запрет веб-сайтов на ваши скребки, то элитные прокси - ваш оптимальный выбор; это заставит вас казаться обычным интернет-пользователем, который вообще не использует прокси.

Кроме того, дополнительной мерой защиты от парсинга является использование вращающихся пользовательских агентов, в которых вы каждый раз отправляете изменяющийся поддельный заголовок, говоря, что вы обычный браузер.

Наконец, PYPROXY является идеальным прокси-решением, которое имеет более 25 миллионов активных IP-адресов, обновляемых каждый день с показателем успеха 99,9%.